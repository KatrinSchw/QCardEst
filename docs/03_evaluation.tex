\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[style=numeric,backend=biber]{biblatex}
\addbibresource{references.bib}

\title{Evaluation of QCardEst/QCardCorr on JOB-light and STATS benchmarks}
\author{Katrin Schwab}
\date{21.12.2025}

\begin{document}

\maketitle

\section{Scope and Constraints}

This document analyzes and extends the evaluation of the QCardEst and QCardCorr models proposed by Winker et al. \cite{winker2025qcardest}.
Before examining the results, it is important to clearly define the scope and constraints of our evaluation. These points help establish the strength of the conclusions presented and clarify what the results do and do not claim.

\subsection{Quantum Simulation Environment}

First, the results are obtained using a quantum circuit simulator rather than real quantum hardware. This has several important implications:

\begin{itemize}
\item \textbf{No hardware noise}: Simulators produce ideal quantum states. The results reflect model capacity and the effectiveness of the hybrid approach, not hardware limitations or noise characteristics.
\item \textbf{Prediction quality focus}: The evaluation focuses on prediction quality, not execution time. We do not claim quantum speedup or runtime advantages. The work evaluates whether quantum models can improve cardinality estimation accuracy.
\end{itemize}

\subsection{Experimental Setup}

Queries join up to 6 tables, matching 6 qubits (one qubit per table; Compact encoding), which keeps the simulation realistic and feasible on today’s quantum hardware, while still being complex enough to stress the model. \cite{winker2025qcardest}

The training process involves a hybrid quantum-classical optimization loop: 
\begin{itemize}
\item A Variational Quantum Circuit (VQC) is executed repeatedly
\item After each execution, parameters are updated using the Adam optimizer with a decaying learning rate
\item The optimization runs for 8000 episodes
\end{itemize}

Each training episode involves:
\begin{itemize}
\item Feature encoding
\item Simulating a \textbf{6-qubit, 16-layer VQC}
\item Measuring the circuit to obtain probability distributions
\item Classical post-processing through various layer types
\item Computing gradients and updating parameters
\end{itemize}

This hybrid optimization loop makes training computationally expensive, but enables end-to-end learning of the quantum-classical pipeline.

\section{Benchmark Descriptions}

Before examining the actual results, it is important to understand what kind of data the model is tested on, because this has a big impact on how difficult the task actually is.

In database systems, not all workloads are equally complex. Some queries follow patterns that optimizers can solve efficiently, while others are intentionally designed to break assumptions and expose weaknesses. To cover both cases, the research group evaluated the QCardEst/QCardCorr models on two different benchmarks: JOB-light and STATS.

Both JOB-light and STATS are widely used benchmarks for evaluating cardinality estimation methods and query optimizers, but they differ significantly in their underlying data and query complexity.

\subsection{JOB-light}

JOB-light \cite{leis2015good} is derived from the real-world IMDB dataset. It contains:
\begin{itemize}
\item Natural correlations between attributes
\item Skewed distributions that reflect realistic data characteristics
\item Real-world query patterns
\end{itemize}

Despite being realistic, these natural patterns are often handled reasonably well by modern optimizers like PostgreSQL, which is why JOB-light is generally considered the easier of the two benchmarks. \cite{jobstatscomparison}

\subsection{STATS}

STATS \cite{han2021cardinality}, in contrast, is based on a synthetic but carefully designed dataset. Its purpose is not realism, but control:
\begin{itemize}
\item Data distributions are constructed to systematically test different types of predicates
\item Designed to test various selectivities and filter combinations
\item Provides controlled stress testing scenarios
\end{itemize}

Because of this controlled design, STATS is typically much harder for traditional optimizers and simpler learned models. It reveals weaknesses that might never appear when working only with naturally correlated real-world data. \cite{postgresdocs}

\subsection{Evaluation Strategy}

Using both benchmarks provides a balanced evaluation, combining realistic workloads with controlled stress testing. We use the same hybrid quantum--classical pipeline for both benchmarks, so any performance differences observed are due to workload characteristics rather than changes in the model architecture or training procedure.

\section{Experimental Results}

This section presents the experimental results comparing QCardEst (cardinality estimation) and QCardCorr (cardinality correction) approaches across nine different classical post-processing layers.

\subsection{JOB-light Benchmark Results}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../analysis/figures/job_light_classical_layers_comparison.png}
\caption{JOB-light benchmark: comparison figure for all classical layers. The chart shows mean error difference for Estimation (blue bars) and Correction (orange bars) approaches. Dashed lines indicate PostgreSQL (black, 2.48) and MSCN (green, 1.35) baseline errors.}
\end{figure}

Figure 1 shows the comparison of all classical layers for the JOB-light benchmark.

\section{JOB-light Benchmark: Detailed Analysis}

\subsection{Overview}

For JOB-light, the \textbf{Threshold} layer achieves the best correction performance with a mean error difference of \textbf{0.39}, representing a remarkable improvement over both baseline estimators. This performance is \textbf{6.37 times better than the PostgreSQL baseline} (2.48) and \textbf{3.47 times better than the MSCN baseline} (1.35). The second-best performer is \textbf{PlaceValueNeg8} with an error of \textbf{0.42}, which is \textbf{5.91 times better than PostgreSQL} and \textbf{3.22 times better than MSCN}.

When examining all correction approaches, the research group observes that in \textbf{6 out of 8 cases where cardinality correction outperforms PostgreSQL, it also outperforms MSCN}. This pattern suggests that the quantum correction mechanisms are capable of systematically improving upon both classical estimators, with the improvement being more pronounced relative to PostgreSQL's estimates.

\subsection{Why Threshold achieves the best performance compared to all other layers for JOB-light}

\subsubsection{Mechanism}

The threshold layer (\texttt{SecondValueThreshold}) uses a gating mechanism based on ReLU activations with a threshold at 0.25:

\begin{verbatim}
posChange = 1 + ReLU(x[0] - 0.25) * x[1] * scalar²
negChange = 1 + ReLU(x[2] - 0.25) * x[3] * scalar²
result = posChange - negChange
\end{verbatim}

\subsubsection{Why It Works Well for Correction}

\begin{enumerate}
\item \textbf{Selective Application}: The threshold mechanism only applies corrections when the quantum output exceeds the threshold, effectively filtering out noise and only making corrections when there is sufficient confidence in the quantum model's output.

\item \textbf{Bidirectional Corrections}: By combining positive and negative changes through subtraction, the threshold layer can handle both overestimations and underestimations from the baseline (PostgreSQL) estimator.

\item \textbf{Stability}: The threshold acts as a regularization mechanism, preventing the model from making unnecessary corrections when the baseline is already accurate. This is particularly important for correction tasks where many queries may already have reasonable estimates.
\end{enumerate}

\subsubsection{Key Observations for JOB-light}

\begin{itemize}
\item \textbf{Correction dominance}: All correction approaches except PlaceValue8, PlaceValueNeg and rationalLog outperform both PostgreSQL and MSCN baselines. Since multiple independent correction layers show the same trend, this shows systematic improvement against the baselines.

\item \textbf{Estimation limitations}: For direct cardinality estimation, only \textbf{Linear} and \textbf{PlaceValue8} perform better than the PostgreSQL baseline. All other layers fail to improve upon PostgreSQL's estimates.

\item \textbf{PlaceValue8 anomaly}: PlaceValue8 shows dramatically different behavior between correction (6.28 error, worst performer) and estimation (1.78 error, second-best). This stems from a fundamental limitation: PlaceValue8 can \textbf{only produce positive numbers}, meaning it can only correct cardinalities by \textbf{increasing them}. Since PostgreSQL estimates often need downward corrections, PlaceValue8 is unable to make the necessary adjustments, leading to poor correction performance. However, for estimation, this constraint is less problematic since cardinalities are inherently positive values.

\item \textbf{PlaceValueNeg8 contrast}: The variant PlaceValueNeg8, which allows for negative numbers, performs excellently for correction (0.42, second-best) but poorly for estimation (8.62). This inverse relationship between PlaceValue8 and PlaceValueNeg8 highlights the importance of \textbf{bidirectional correction capability} for improving baseline estimates, while simpler positive-only representations work better for absolute value estimation.
\end{itemize}

\section{STATS Benchmark: Detailed Analysis}

\subsection{Overview}

For STATS, the performance landscape differs significantly from JOB-light. The best correction performance is achieved by \textbf{RationalLog} with a mean error difference of \textbf{0.32}, which is \textbf{8.66 times better than the PostgreSQL baseline} (2.77). The second-best is \textbf{Rational} with an error of \textbf{1.04}, representing a \textbf{2.67 times improvement} over PostgreSQL. Notably, the Threshold layer, which dominated JOB-light, achieves 1.96 error (still better than PostgreSQL, but not the best).

This shift in optimal layer choice between benchmarks suggests that \textbf{the effectiveness of correction layers is sensitive to query workload characteristics}. 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{../analysis/figures/stats_classical_layers_comparison.png}
\caption{STATS benchmark: comparison figure for all classical layers. The chart shows mean error difference for Estimation (blue bars) and Correction (orange bars) approaches. Dashed line indicates PostgreSQL (black, 2.77) baseline error.}
\end{figure}

Figure 2 shows the comparison of all classical layers for the STATS benchmark.

\subsection{Key Findings}

The STATS benchmark reveals several important patterns:

\begin{enumerate}
\item \textbf{Rational layers excel}: RationalLog and Rational achieve the best correction performance, with RationalLog showing an exceptional 8.66× improvement over PostgreSQL. This represents the largest improvement factor observed across both benchmarks.

\item \textbf{Linear leads estimation}: As in JOB-light, Linear maintains its position as the best estimation layer (1.65 vs 1.43 in JOB-light), and closely followed by PlaceValue8 (2.97), it is the only layer that outperforms the PostgreSQL baseline for direct estimation.

\item \textbf{Higher baseline error}: PostgreSQL baseline error is 2.77 for STATS vs 2.48 for JOB-light, indicating that STATS queries may be inherently more challenging for classical estimators, yet still allowing for significant quantum-based improvements.

\item \textbf{Correction vs Estimation gap}: The gap between best correction (0.32) and best estimation in STATS is 1.33, which is larger than the gap in JOB-light (1.04). For STATS, the gap is much larger because this benchmark is harder for classical methods like PostgreSQL and they make bigger errors. Correcting bigger errors gives the largest learning benefit, which is why the correction approach achieves very low error on STATS. 
\end{enumerate}

\section{Comparison: JOB-light vs STATS}

\subsection{Absolute Performance Differences}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Metric & JOB-light & STATS & Difference \\
\midrule
PostgreSQL Baseline & 2.48 & 2.77 & +0.29 (STATS harder) \\
Best Correction (threshold/rationalLog) & 0.39 & 0.32 & -0.07 (STATS better) \\
Best Estimation (linear) & 1.43 & 1.65 & +0.22 (STATS harder) \\
Gap (Best Correction - Best Estimation) & 1.04 & 1.33 & +0.29 (STATS larger gap) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Differences}

\begin{enumerate}
\item \textbf{Error Magnitude}: STATS shows overall higher errors across all layers, suggesting the benchmark is inherently more challenging. 

\item \textbf{Correction Advantage}: The advantage of correction over estimation is actually larger in STATS (1.33) compared to JOB-light (1.04). This suggests that:
\begin{itemize}
\item Correction approaches can achieve even larger improvements in STATS (RationalLog: 8.66× better than PostgreSQL) compared to JOB-light (Threshold: 6.37× better)
\item The best correction in STATS (0.32) is actually better than the best correction in JOB-light (0.39), despite STATS being more challenging overall
\item Different layer architectures (RationalLog vs Threshold) are optimal for different workloads
\end{itemize}

\item \textbf{Layer Robustness}: The relative ranking of layers shows some variation between benchmarks:
\begin{itemize}
\item \textbf{Correction}: Threshold excels in JOB-light while RationalLog dominates STATS, though PlaceValueNeg8 remains strong in both
\item \textbf{Estimation}: Linear and PlaceValue8 maintain their leading positions in both benchmarks
\item This variation suggests that optimal layer choice should consider workload characteristics
\end{itemize}

\item \textbf{Baseline Quality}: Both benchmarks have similar PostgreSQL baseline errors (2.48 vs 2.77), yet correction approaches achieve larger improvements in STATS compared to JOB-light, demonstrating that optimal layer choice can unlock significant correction potential.
\end{enumerate}

\subsection{Similarities}

\begin{enumerate}
\item \textbf{Linear estimation dominance}: Linear layer consistently performs best for estimation in both benchmarks (1.43 JOB-light, 1.65 STATS), validating its simplicity and effectiveness.

\item \textbf{PlaceValueNeg8 reliability}: PlaceValueNeg8 consistently ranks high for correction, showing its reliability across different workloads.

\item \textbf{PlaceValue8 constraint}: PlaceValue8 consistently fails at correction due to its positive-only output constraint.

\item \textbf{Estimation challenges}: Only Linear and PlaceValue8 outperform or come very close to PostgreSQL for estimation, while most other layers struggle with absolute value estimation in both benchmarks.
\end{enumerate}

\section{When to Pick Which Layer (Rule of Thumb)}

\subsection{For Correction Tasks}

\textbf{For JOB-light workloads: Threshold}
\begin{itemize}
\item Best performance (0.39, 6.37× better than PostgreSQL)
\item Robust and interpretable selective correction mechanism
\item Use when: Working with JOB-light-style queries and PostgreSQL baseline
\end{itemize}

\textbf{For STATS workloads: RationalLog}
\begin{itemize}
\item Best performance (0.32, 8.66× better than PostgreSQL)
\item Exceptional improvement factor
\item Use when: Working with STATS-style queries and need maximum correction accuracy
\end{itemize}

\subsection{For Estimation Tasks}

\textbf{Linear}
\begin{itemize}
\item Best performance for estimation in both benchmarks (1.43 JOB-light, 1.65 STATS)
\item Use when: You must do direct estimation without a baseline or when the baseline is not good enough
\end{itemize}

\subsection{Cross-cutting Insights and Principles}

\begin{itemize}
\item \textbf{Cardinality correction is more robust than direct estimation}: The consistent superiority of correction approaches across all layers and benchmarks demonstrates that leveraging baseline estimates and applying selective adjustments is fundamentally more effective than estimating absolute cardinalities from scratch.

\item \textbf{Expressiveness of the classical post-processing layer is critical}: The dramatic performance differences between layers (e.g., Threshold vs. PlaceValue8 for correction) show that the choice of classical post-processing architecture is as important as the quantum circuit design itself.
\end{itemize}

\subsection{Key Takeaways}

\begin{itemize}
\item \textbf{Hybrid quantum--classical models outperform classical baselines}: The 6--8× improvements over PostgreSQL and 3--4× improvements over MSCN show that quantum-enhanced approaches can provide substantial accuracy gains when properly integrated with classical methods.

\item \textbf{Cardinality correction is a realistic near-term application}: Since the hybrid models rely on compact encoding, shallow variational quantum circuits with a small number of qubits, quantum correction of classical cardinality estimates is theoretically feasible on current and near-term quantum hardware. QCardCorr aligns well with the capabilities of today’s noise-prone quantum processors.
\end{itemize}

\section{Future Work}

Building on the insights from this evaluation, several promising directions for future research emerge across different time horizons.

\subsection{Short-term}

\begin{itemize}
\item \textbf{Evaluation on larger benchmarks}: Extending the evaluation to benchmarks with more complex query patterns, larger numbers of tables, and more diverse data distributions would provide further validation of the approach and help identify scalability limits.

\item \textbf{Automatic learning of classical post-processing layers}: Rather than manually selecting from predefined layer architectures, developing methods to automatically learn or discover optimal classical post-processing layers could further improve performance.
\end{itemize}

\subsection{Mid-term}

\begin{itemize}
\item \textbf{Integration into real DBMS pipelines}: Moving from simulation to integration with actual database management systems would enable evaluation of end-to-end query optimization performance and assessment of practical deployment considerations.

\item \textbf{Execution on real quantum hardware}: Transitioning from quantum simulators to real quantum hardware would reveal the impact of noise, gate errors, and other hardware limitations on model performance, providing crucial insights for practical deployment.
\end{itemize}

\subsection{Long-term}

\begin{itemize}
\item \textbf{Joint optimization of join order and cardinality estimation}: A quantum-enhanced model could, in principle, reason over entire query structures rather than individual estimates. This would enable joint optimization of join order selection and cardinality estimation, potentially leading to more globally optimal query plans.

\item \textbf{Deeper and more expressive quantum models as hardware scales}: As quantum hardware continues to improve in terms of qubit count, gate fidelity, and coherence times, opportunities emerge for deploying deeper and more expressive quantum circuits. This could enable more sophisticated feature encodings and potentially unlock new capabilities for query optimization.
\end{itemize}


\printbibliography
\end{document}

